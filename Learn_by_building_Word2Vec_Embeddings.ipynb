{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Learn-by-building Word2Vec Embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R43iFeorDMt-",
        "colab_type": "text"
      },
      "source": [
        "# **Learn-by-building Word2Vec Embeddings**\n",
        "\n",
        "In a team of 2 people, create a Word2Vec model by using `gensim` and/or `elang` package. You may use or gather your own dataset to:\n",
        "\n",
        "- Build a word embedding from text data (NLP), or\n",
        "- Build a recommender system (Non-NLP), or\n",
        "- Anything else that have a sequential properties (Non-NLP).\n",
        "\n",
        "The rubrics are as follow:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRCEcG-ZFZ6G",
        "colab_type": "text"
      },
      "source": [
        "## Collect the Data\n",
        "\n",
        "- What is the data all about? Give short explanation about the data.\n",
        "- Where do you get the data from?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPFuTj4bGCDH",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "### NLP\n",
        "Text data needs to be cleanse before we feed it into a model so that it can capture the semantic of word correctly.\n",
        "- What are the necessary cleansing steps need to be performed?\n",
        "- Do you need to remove the stopwords or perform stemming/lemmatizing?\n",
        "- Have you confirm that the sentence is in the form of a \"list of lists of words\"?\n",
        "\n",
        "### Non-NLP\n",
        "This step may be vary depending on the case, but on general:\n",
        "- Is there any missing value? If yes, then how do you handle it?\n",
        "- Have you confirm that the data type is already appropriate?\n",
        "- Do you need to perform train-test splitting?\n",
        "- What feature do you use to get the embedding vectors?\n",
        "- Have you confirm that the feature is in the form of a \"list of lists\"?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkTKCncTGH_t",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "\n",
        "Consider these following parameters for training the model:\n",
        "- What are the dimensions of the embedding vector?\n",
        "- What is the maximum distance between the context and target feature?\n",
        "- What is the minimum frequency for a feature to be considered as a vocabulary?\n",
        "- Are you using Skip-Gram or CBOW architecture? Why?\n",
        "- Which training optimization do you use?\n",
        "- How many epochs do you let the model to train?\n",
        "\n",
        "After training is done:\n",
        "- What is the final size of your vocabulary? Is it far different from the unique count of the original data?\n",
        "\n",
        "Tips on training Word2Vec model:\n",
        "- Use `logging` to monitor the training process.\n",
        "- Use `Callback` to monitor the loss of each epoch.\n",
        "- Reproducibility is quite hard to maintain, so don't forget to always save your model after the training process is done."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOsmtkuNGXbD",
        "colab_type": "text"
      },
      "source": [
        "## Visualize\n",
        "\n",
        "It is always quite helpful to visualize the embeddings that you have created. The dimension of the embedding may be tens to hundreds, whereas humans are limited to see up to three dimensions. So, the dimensions of the vectors must be reduced.\n",
        "- Which dimensionality reduction algorithm do you use?\n",
        "- Is there any interesting pattern that could be seen from the visualization? Please elaborate.\n",
        "\n",
        "Tips:\n",
        "- You may use `elang` package for this section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-FkANyJGOxG",
        "colab_type": "text"
      },
      "source": [
        "## Use the Word Vectors\n",
        "\n",
        "You can use several method provided by `gensim` to see how your model is performing.\n",
        "\n",
        "### NLP\n",
        "- Choose any words from the dictionary and list out several similar words. Does it semantically makes any sense?\n",
        "- Does your model able to point out one word that has different context among the other words in a list?\n",
        "- Does your model able to capture a semantic relation between words?\n",
        "\n",
        "### Non-NLP\n",
        "This step is loosely-defined depending on the case, but consider the following:\n",
        "- Make sure to use the similarity score on your analysis.\n",
        "- How you can use your model to compute the similarity based on multiple feature?\n",
        "\n",
        "Tips:\n",
        "- If the output doesn't make any sense at all, consider training your model again by adding more data or do parameter tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5D7uHULuNyN",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "### NLP\n",
        "- How does your model performance capturing the semantics of words?\n",
        "- What are other task(s) that can be done after you successfully represent words into vectors?\n",
        "\n",
        "### Non-NLP\n",
        "- Does your model work according to your expectation?\n",
        "- What might be the next step after the model is obtained?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_E0dCKchFyG",
        "colab_type": "text"
      },
      "source": [
        "# Dataset Reference\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vMmFltIvqYl",
        "colab_type": "text"
      },
      "source": [
        "## NLP\n",
        "\n",
        "**Data provided on `gensim`: https://github.com/RaRe-Technologies/gensim-data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXnljFO5htU-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_colwidth', -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnRifVs2hBEo",
        "colab_type": "code",
        "outputId": "7dd08c72-9d15-48c0-b00f-68fa125c45bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import gensim.downloader as api\n",
        "info = api.info()\n",
        "corpora_list = [\"quora-duplicate-questions\", \"text8\", \"fake-news\", \"20-newsgroups\"]\n",
        "pd.DataFrame(info['corpora']).T.loc[corpora_list,:]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num_records</th>\n",
              "      <th>record_format</th>\n",
              "      <th>file_size</th>\n",
              "      <th>reader_code</th>\n",
              "      <th>license</th>\n",
              "      <th>fields</th>\n",
              "      <th>description</th>\n",
              "      <th>checksum</th>\n",
              "      <th>file_name</th>\n",
              "      <th>read_more</th>\n",
              "      <th>parts</th>\n",
              "      <th>checksum-0</th>\n",
              "      <th>checksum-1</th>\n",
              "      <th>checksum-2</th>\n",
              "      <th>checksum-3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>quora-duplicate-questions</th>\n",
              "      <td>404290</td>\n",
              "      <td>dict</td>\n",
              "      <td>21684784</td>\n",
              "      <td>https://github.com/RaRe-Technologies/gensim-data/releases/download/quora-duplicate-questions/__init__.py</td>\n",
              "      <td>probably https://www.quora.com/about/tos</td>\n",
              "      <td>{'question1': 'the full text of each question', 'question2': 'the full text of each question', 'qid1': 'unique ids of each question', 'qid2': 'unique ids of each question', 'id': 'the id of a training set question pair', 'is_duplicate': 'the target variable, set to 1 if question1 and question2 have essentially the same meaning, and 0 otherwise'}</td>\n",
              "      <td>Over 400,000 lines of potential question duplicate pairs. Each line contains IDs for each question in the pair, the full text for each question, and a binary value that indicates whether the line contains a duplicate pair or not.</td>\n",
              "      <td>d7cfa7fbc6e2ec71ab74c495586c6365</td>\n",
              "      <td>quora-duplicate-questions.gz</td>\n",
              "      <td>[https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs]</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>text8</th>\n",
              "      <td>1701</td>\n",
              "      <td>list of str (tokens)</td>\n",
              "      <td>33182058</td>\n",
              "      <td>https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py</td>\n",
              "      <td>not found</td>\n",
              "      <td>NaN</td>\n",
              "      <td>First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.</td>\n",
              "      <td>68799af40b6bda07dfa47a32612e5364</td>\n",
              "      <td>text8.gz</td>\n",
              "      <td>[http://mattmahoney.net/dc/textdata.html]</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fake-news</th>\n",
              "      <td>12999</td>\n",
              "      <td>dict</td>\n",
              "      <td>20102776</td>\n",
              "      <td>https://github.com/RaRe-Technologies/gensim-data/releases/download/fake-news/__init__.py</td>\n",
              "      <td>https://creativecommons.org/publicdomain/zero/1.0/</td>\n",
              "      <td>{'crawled': 'date the story was archived', 'ord_in_thread': '', 'published': 'date published', 'participants_count': 'number of participants', 'shares': 'number of Facebook shares', 'replies_count': 'number of replies', 'main_img_url': 'image from story', 'spam_score': 'data from webhose.io', 'uuid': 'unique identifier', 'language': 'data from webhose.io', 'title': 'title of story', 'country': 'data from webhose.io', 'domain_rank': 'data from webhose.io', 'author': 'author of story', 'comments': 'number of Facebook comments', 'site_url': 'site URL from BS detector', 'text': 'text of story', 'thread_title': '', 'type': 'type of website (label from BS detector)', 'likes': 'number of Facebook likes'}</td>\n",
              "      <td>News dataset, contains text and metadata from 244 websites and represents 12,999 posts in total from a specific window of 30 days. The data was pulled using the webhose.io API, and because it's coming from their crawler, not all websites identified by their BS Detector are present in this dataset. Data sources that were missing a label were simply assigned a label of 'bs'. There are (ostensibly) no genuine, reliable, or trustworthy news sources represented in this dataset (so far), so don't trust anything you read.</td>\n",
              "      <td>5e64e942df13219465927f92dcefd5fe</td>\n",
              "      <td>fake-news.gz</td>\n",
              "      <td>[https://www.kaggle.com/mrisdal/fake-news]</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20-newsgroups</th>\n",
              "      <td>18846</td>\n",
              "      <td>dict</td>\n",
              "      <td>14483581</td>\n",
              "      <td>https://github.com/RaRe-Technologies/gensim-data/releases/download/20-newsgroups/__init__.py</td>\n",
              "      <td>not found</td>\n",
              "      <td>{'topic': 'name of topic (20 variant of possible values)', 'set': 'marker of original split (possible values 'train' and 'test')', 'data': '', 'id': 'original id inferred from folder name'}</td>\n",
              "      <td>The notorious collection of approximately 20,000 newsgroup posts, partitioned (nearly) evenly across 20 different newsgroups.</td>\n",
              "      <td>c92fd4f6640a86d5ba89eaad818a9891</td>\n",
              "      <td>20-newsgroups.gz</td>\n",
              "      <td>[http://qwone.com/~jason/20Newsgroups/]</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          num_records         record_format file_size  \\\n",
              "quora-duplicate-questions  404290      dict                  21684784   \n",
              "text8                      1701        list of str (tokens)  33182058   \n",
              "fake-news                  12999       dict                  20102776   \n",
              "20-newsgroups              18846       dict                  14483581   \n",
              "\n",
              "                                                                                                                        reader_code  \\\n",
              "quora-duplicate-questions  https://github.com/RaRe-Technologies/gensim-data/releases/download/quora-duplicate-questions/__init__.py   \n",
              "text8                      https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py                       \n",
              "fake-news                  https://github.com/RaRe-Technologies/gensim-data/releases/download/fake-news/__init__.py                   \n",
              "20-newsgroups              https://github.com/RaRe-Technologies/gensim-data/releases/download/20-newsgroups/__init__.py               \n",
              "\n",
              "                                                                      license  \\\n",
              "quora-duplicate-questions  probably https://www.quora.com/about/tos             \n",
              "text8                      not found                                            \n",
              "fake-news                  https://creativecommons.org/publicdomain/zero/1.0/   \n",
              "20-newsgroups              not found                                            \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       fields  \\\n",
              "quora-duplicate-questions  {'question1': 'the full text of each question', 'question2': 'the full text of each question', 'qid1': 'unique ids of each question', 'qid2': 'unique ids of each question', 'id': 'the id of a training set question pair', 'is_duplicate': 'the target variable, set to 1 if question1 and question2 have essentially the same meaning, and 0 otherwise'}                                                                                                                                                                                                                                                                                                                                                                          \n",
              "text8                      NaN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
              "fake-news                  {'crawled': 'date the story was archived', 'ord_in_thread': '', 'published': 'date published', 'participants_count': 'number of participants', 'shares': 'number of Facebook shares', 'replies_count': 'number of replies', 'main_img_url': 'image from story', 'spam_score': 'data from webhose.io', 'uuid': 'unique identifier', 'language': 'data from webhose.io', 'title': 'title of story', 'country': 'data from webhose.io', 'domain_rank': 'data from webhose.io', 'author': 'author of story', 'comments': 'number of Facebook comments', 'site_url': 'site URL from BS detector', 'text': 'text of story', 'thread_title': '', 'type': 'type of website (label from BS detector)', 'likes': 'number of Facebook likes'}   \n",
              "20-newsgroups              {'topic': 'name of topic (20 variant of possible values)', 'set': 'marker of original split (possible values 'train' and 'test')', 'data': '', 'id': 'original id inferred from folder name'}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        description  \\\n",
              "quora-duplicate-questions  Over 400,000 lines of potential question duplicate pairs. Each line contains IDs for each question in the pair, the full text for each question, and a binary value that indicates whether the line contains a duplicate pair or not.                                                                                                                                                                                                                                                                                                      \n",
              "text8                      First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.                                                                                                                                                                                                                                                                                                                                                                                                    \n",
              "fake-news                  News dataset, contains text and metadata from 244 websites and represents 12,999 posts in total from a specific window of 30 days. The data was pulled using the webhose.io API, and because it's coming from their crawler, not all websites identified by their BS Detector are present in this dataset. Data sources that were missing a label were simply assigned a label of 'bs'. There are (ostensibly) no genuine, reliable, or trustworthy news sources represented in this dataset (so far), so don't trust anything you read.   \n",
              "20-newsgroups              The notorious collection of approximately 20,000 newsgroup posts, partitioned (nearly) evenly across 20 different newsgroups.                                                                                                                                                                                                                                                                                                                                                                                                              \n",
              "\n",
              "                                                   checksum  \\\n",
              "quora-duplicate-questions  d7cfa7fbc6e2ec71ab74c495586c6365   \n",
              "text8                      68799af40b6bda07dfa47a32612e5364   \n",
              "fake-news                  5e64e942df13219465927f92dcefd5fe   \n",
              "20-newsgroups              c92fd4f6640a86d5ba89eaad818a9891   \n",
              "\n",
              "                                              file_name  \\\n",
              "quora-duplicate-questions  quora-duplicate-questions.gz   \n",
              "text8                      text8.gz                       \n",
              "fake-news                  fake-news.gz                   \n",
              "20-newsgroups              20-newsgroups.gz               \n",
              "\n",
              "                                                                                     read_more  \\\n",
              "quora-duplicate-questions  [https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs]   \n",
              "text8                      [http://mattmahoney.net/dc/textdata.html]                             \n",
              "fake-news                  [https://www.kaggle.com/mrisdal/fake-news]                            \n",
              "20-newsgroups              [http://qwone.com/~jason/20Newsgroups/]                               \n",
              "\n",
              "                          parts checksum-0 checksum-1 checksum-2 checksum-3  \n",
              "quora-duplicate-questions  1     NaN        NaN        NaN        NaN        \n",
              "text8                      1     NaN        NaN        NaN        NaN        \n",
              "fake-news                  1     NaN        NaN        NaN        NaN        \n",
              "20-newsgroups              1     NaN        NaN        NaN        NaN        "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQhecEHKhQN1",
        "colab_type": "code",
        "outputId": "6bf83464-3917-4817-9843-9f143607d74d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# how to use\n",
        "news = api.load(\"20-newsgroups\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.1% 13.8/13.8MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RBcHI_tBI5aI"
      },
      "source": [
        "**NLTK Corpus: https://www.nltk.org/book/ch02.html**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9piRcw2ZhCA",
        "colab_type": "code",
        "outputId": "ae81903b-6cfc-40e3-d270-32883759eff2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download(['gutenberg', 'punkt'])\n",
        "from nltk.corpus import gutenberg"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnfYX3jJaRZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def corporaStat(corpora_name):\n",
        "  file_list = []\n",
        "  for fileid in corpora_name.fileids():\n",
        "    file_dict = {\n",
        "        \"Filename\": fileid,\n",
        "        \"Character Count\": len(corpora_name.raw(fileid)),\n",
        "        \"Word Count\": len(corpora_name.words(fileid)),\n",
        "        \"Sentence Count\": len(corpora_name.sents(fileid)),\n",
        "        \"Vocabulary Count\": len(set(w.lower() for w in corpora_name.words(fileid)))\n",
        "    }\n",
        "    file_list.append(file_dict)\n",
        "  return pd.DataFrame(file_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_FVi5c4cKs2",
        "colab_type": "code",
        "outputId": "8634fbf4-8f8a-4795-acc3-31155264cbfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        }
      },
      "source": [
        "corporaStat(gutenberg)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Filename</th>\n",
              "      <th>Character Count</th>\n",
              "      <th>Word Count</th>\n",
              "      <th>Sentence Count</th>\n",
              "      <th>Vocabulary Count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>austen-emma.txt</td>\n",
              "      <td>887071</td>\n",
              "      <td>192427</td>\n",
              "      <td>7752</td>\n",
              "      <td>7344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>austen-persuasion.txt</td>\n",
              "      <td>466292</td>\n",
              "      <td>98171</td>\n",
              "      <td>3747</td>\n",
              "      <td>5835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>austen-sense.txt</td>\n",
              "      <td>673022</td>\n",
              "      <td>141576</td>\n",
              "      <td>4999</td>\n",
              "      <td>6403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bible-kjv.txt</td>\n",
              "      <td>4332554</td>\n",
              "      <td>1010654</td>\n",
              "      <td>30103</td>\n",
              "      <td>12767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>blake-poems.txt</td>\n",
              "      <td>38153</td>\n",
              "      <td>8354</td>\n",
              "      <td>438</td>\n",
              "      <td>1535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>bryant-stories.txt</td>\n",
              "      <td>249439</td>\n",
              "      <td>55563</td>\n",
              "      <td>2863</td>\n",
              "      <td>3940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>burgess-busterbrown.txt</td>\n",
              "      <td>84663</td>\n",
              "      <td>18963</td>\n",
              "      <td>1054</td>\n",
              "      <td>1559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>carroll-alice.txt</td>\n",
              "      <td>144395</td>\n",
              "      <td>34110</td>\n",
              "      <td>1703</td>\n",
              "      <td>2636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>chesterton-ball.txt</td>\n",
              "      <td>457450</td>\n",
              "      <td>96996</td>\n",
              "      <td>4779</td>\n",
              "      <td>8335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>chesterton-brown.txt</td>\n",
              "      <td>406629</td>\n",
              "      <td>86063</td>\n",
              "      <td>3806</td>\n",
              "      <td>7794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>chesterton-thursday.txt</td>\n",
              "      <td>320525</td>\n",
              "      <td>69213</td>\n",
              "      <td>3742</td>\n",
              "      <td>6349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>edgeworth-parents.txt</td>\n",
              "      <td>935158</td>\n",
              "      <td>210663</td>\n",
              "      <td>10230</td>\n",
              "      <td>8447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>melville-moby_dick.txt</td>\n",
              "      <td>1242990</td>\n",
              "      <td>260819</td>\n",
              "      <td>10059</td>\n",
              "      <td>17231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>milton-paradise.txt</td>\n",
              "      <td>468220</td>\n",
              "      <td>96825</td>\n",
              "      <td>1851</td>\n",
              "      <td>9021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>shakespeare-caesar.txt</td>\n",
              "      <td>112310</td>\n",
              "      <td>25833</td>\n",
              "      <td>2163</td>\n",
              "      <td>3032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>shakespeare-hamlet.txt</td>\n",
              "      <td>162881</td>\n",
              "      <td>37360</td>\n",
              "      <td>3106</td>\n",
              "      <td>4716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>shakespeare-macbeth.txt</td>\n",
              "      <td>100351</td>\n",
              "      <td>23140</td>\n",
              "      <td>1907</td>\n",
              "      <td>3464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>whitman-leaves.txt</td>\n",
              "      <td>711215</td>\n",
              "      <td>154883</td>\n",
              "      <td>4250</td>\n",
              "      <td>12452</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   Filename  Character Count  Word Count  Sentence Count  \\\n",
              "0   austen-emma.txt          887071           192427      7752             \n",
              "1   austen-persuasion.txt    466292           98171       3747             \n",
              "2   austen-sense.txt         673022           141576      4999             \n",
              "3   bible-kjv.txt            4332554          1010654     30103            \n",
              "4   blake-poems.txt          38153            8354        438              \n",
              "5   bryant-stories.txt       249439           55563       2863             \n",
              "6   burgess-busterbrown.txt  84663            18963       1054             \n",
              "7   carroll-alice.txt        144395           34110       1703             \n",
              "8   chesterton-ball.txt      457450           96996       4779             \n",
              "9   chesterton-brown.txt     406629           86063       3806             \n",
              "10  chesterton-thursday.txt  320525           69213       3742             \n",
              "11  edgeworth-parents.txt    935158           210663      10230            \n",
              "12  melville-moby_dick.txt   1242990          260819      10059            \n",
              "13  milton-paradise.txt      468220           96825       1851             \n",
              "14  shakespeare-caesar.txt   112310           25833       2163             \n",
              "15  shakespeare-hamlet.txt   162881           37360       3106             \n",
              "16  shakespeare-macbeth.txt  100351           23140       1907             \n",
              "17  whitman-leaves.txt       711215           154883      4250             \n",
              "\n",
              "    Vocabulary Count  \n",
              "0   7344              \n",
              "1   5835              \n",
              "2   6403              \n",
              "3   12767             \n",
              "4   1535              \n",
              "5   3940              \n",
              "6   1559              \n",
              "7   2636              \n",
              "8   8335              \n",
              "9   7794              \n",
              "10  6349              \n",
              "11  8447              \n",
              "12  17231             \n",
              "13  9021              \n",
              "14  3032              \n",
              "15  4716              \n",
              "16  3464              \n",
              "17  12452             "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PhA27p6wuP5",
        "colab_type": "text"
      },
      "source": [
        "## Non-NLP\n",
        "\n",
        "1. Online retail dataset (the one we used in the internal training):\n",
        "- http://archive.ics.uci.edu/ml/datasets/Online+Retail\n",
        "- http://archive.ics.uci.edu/ml/datasets/Online+Retail+II\n",
        "\n",
        "2. Instacart\n",
        "- Dataset: https://www.kaggle.com/c/instacart-market-basket-analysis/overview\n",
        "- Reference: https://omarito.me/word2vec-product-recommendations/\n",
        "\n",
        "3. Music Listening History\n",
        "- Full Dataset: http://ocelma.net/MusicRecommendationDataset/lastfm-1K.html (Almost 2.5 GB)\n",
        "- Subsetted Dataset (100 users): https://github.com/tomytjandra/word2vec-embeddings/tree/master/dataset/lastfm-dataset (289 MB)"
      ]
    }
  ]
}